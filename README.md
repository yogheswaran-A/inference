# inference
To DO: INT8 quantization of 1 layer of nanoGPT
relevant papers:
Training Deep Nets with Sublinear Memory Cost : https://arxiv.org/pdf/1604.06174
QLORA: https://arxiv.org/pdf/2305.14314   
Qunatization visual guide: https://www.maartengrootendorst.com/blog/quantization/   
Quantization, pruning, etc: https://intellabs.github.io/distiller/algo_quantization.html    
Fine-tuning LLMs to 1.58bit: extreme quantization made easy: https://huggingface.co/blog/1_58_llm_extreme_quantization   
Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT: https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/   
Quantization in practice: https://pytorch.org/blog/quantization-in-practice/   
Quantization, Lei Mao's blog(with codes): https://leimao.github.io/article/Neural-Networks-Quantization/#Introduction   
Techniques: GPTQ, AWQ, NF4, HQQ, GGUF file format based, sharding.   
https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html#how_to_optimize   
Int8 matrix multiplication paper: https://arxiv.org/abs/1712.05877   
code: https://github.com/google/gemmlowp/blob/master/doc/quantization_example.cc   
https://github.com/google/gemmlowp/blob/master/doc/quantization.md      

